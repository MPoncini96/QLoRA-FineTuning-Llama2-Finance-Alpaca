{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmjF86khZf_W"
      },
      "source": [
        "### Matthew Poncini\n",
        "**CSE 590 Special Topics: GenAI**\n",
        "**Homework 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJeyzGrZre2"
      },
      "source": [
        "### Step 0: Environment Setup and Library Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W617icS06iHM"
      },
      "source": [
        "helps prevent memory fragmentation issues when loading large models or training with quantized weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCF3r6J6tS5A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX5UjUZT6ng9"
      },
      "source": [
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "9hILpxs7rc9k",
        "outputId": "00324311-7ddc-4309-c220-abcf9c066ae0"
      },
      "outputs": [],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "FtNIyjtIBvuU",
        "outputId": "00c39719-5d04-466b-b11c-ae6fcc955a44"
      },
      "outputs": [],
      "source": [
        "!pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "PrzvhiKntMMo",
        "outputId": "89c215c7-35c1-478a-ba36-38b5728bdb2f"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "Y7DhoSXCssxL",
        "outputId": "2e1fc593-5447-4c73-833b-5074f2ee1ed6"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "DB0MJCLnBi9K",
        "outputId": "fa05097d-7911-43f9-d66d-cf1e89c67224"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "U3DrmKSWBy96",
        "outputId": "bfa2e857-053f-4b3b-9226-d69c7f52902c"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "Fo-CR1YxB97j",
        "outputId": "fc01b89e-79b8-4783-f6e5-8e6db0e3918a"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RUo8Ym4G3os"
      },
      "source": [
        "Checks if GPU can be ran"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "78xA8eMSdYa6",
        "outputId": "1386f424-f0c8-484f-adfa-6ac41d16b778"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlVvQHYke14g"
      },
      "source": [
        "Log-in to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WLeBFOQ9XaUw",
        "outputId": "0a8af929-1aa9-43c5-bfc3-4cdf93c753a7"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = \"insert_hugging_face_token\"\n",
        "!huggingface-cli login --token $HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUIRk8ebZqQq"
      },
      "source": [
        "### Step 1:  Choose a downstream task and select a suitable dataset for fine-tuning. From the dataset, sample 1,000 examples for training and a separate, non-overlapping 300 examples for testing.\n",
        "\n",
        "Chosen dataset for your fine-tuning task: finance-alpaca\n",
        "\n",
        "https://huggingface.co/datasets/gbharti/finance-alpaca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cM9wdhbpXnBW"
      },
      "outputs": [],
      "source": [
        "dataset = \"gbharti/finance-alpaca\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "16b578679e0f4dce8b9a16bd1c9d36b1",
            "f0bb28bb90594d0a80ea12671452f5a7",
            "eb8f0893e46041528074fdffcc231a78"
          ]
        },
        "id": "_81IXpX4AOf5",
        "outputId": "9d468a2e-09cf-4559-9ec1-c47aace98b0a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gbharti/finance-alpaca\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY05akLZ6yga"
      },
      "source": [
        "Make dataset viewable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0RUDqbWUn3yp",
        "outputId": "3b2a951e-7464-42cc-dbb9-3dc649736a9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = dataset[\"train\"].to_pandas().iloc[:1300]\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qMF_ymOwADT"
      },
      "source": [
        "Select 300 test sample from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ciHfne7kpC_3"
      },
      "outputs": [],
      "source": [
        "test_sample = df.iloc[1000:1300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl_p3KBXqJe7"
      },
      "source": [
        "Select 1,000 training samples from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nlz9kEHlqJNR"
      },
      "outputs": [],
      "source": [
        "train_sample = df.iloc[0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjXw0b-_jRle"
      },
      "source": [
        "### Step 2: Select a pre-trained language model to work with.\n",
        "\n",
        "Chosen pre-trained language model: meta-llama/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "-0x4_r7ags1J"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw11_Ai67B2e"
      },
      "source": [
        "sets up a configuration for loading a quantized version of a large language model using the BitsAndBytes library from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5QEvZZEzg9uG"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y8rve7dwTDj"
      },
      "source": [
        " loads and configures the tokenizer associated with meta-llama/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "cEOaRmTzwK_1",
        "outputId": "c7aeeb9f-5ee7-43e4-ac33-4e4dc2a95edd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyS0NWffQQ_6"
      },
      "source": [
        " loads a quantized large language model onto a GPU using Hugging Face Transformers and BitsAndBytes, ready for fine-tuning or inference, while minimizing memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iw2AdBFsjzsa"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model  = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B_-cjbK7537"
      },
      "source": [
        "Low-Rank Adaptation configuration, fine-tunes only a small number of parameters while freezing the rest of the large model. This significantly reduces memory and compute requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0ZOoXrRRub2G"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p50fjzG8tCqH"
      },
      "source": [
        "### Step 3: Evaluate the pre-trained model on the selected dataset to establish a baseline using an appropriate metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ0Uy9KZl6-e"
      },
      "source": [
        "Evaluate Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TU5J-9d6l1Dx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# makes the generated outputs clean for ROUGE evaluation and display.\n",
        "def extract_response(text):\n",
        "    if \"### Response:\" in text:\n",
        "        return text.split(\"### Response:\")[-1].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# Evaluates a pretrained model on a test set using a generation + ROUGE scoring loop.\n",
        "def evaluate_model(model, tokenizer, prompts, references, batch_size=1, max_new_tokens=64):\n",
        "    pretrained_metric = load(\"rouge\")\n",
        "    predictions = []\n",
        "    records = []\n",
        "\n",
        "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "        batch_refs = references[i:i+batch_size]\n",
        "\n",
        "        # Tokenizes input prompts with truncation and padding\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(model.device)\n",
        "\n",
        "        #  produce text responses (ensures no gradients are computed — saving memory)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "              input_ids=inputs[\"input_ids\"],\n",
        "              attention_mask=inputs[\"attention_mask\"],\n",
        "              do_sample=True,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              max_new_tokens=64,\n",
        "              pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Converts model token outputs back into strings.\n",
        "        batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        batch_preds = [extract_response(p) for p in batch_preds]\n",
        "\n",
        "\n",
        "        # Metric Calculation and Result Collection\n",
        "        for prompt_text, pred, ref in zip(batch_prompts, batch_preds, batch_refs):\n",
        "            pretrained_metric.add(prediction=pred, reference=ref)\n",
        "            predictions.append(pred)\n",
        "            records.append({\n",
        "                \"prompt\": prompt_text,\n",
        "                \"reference\": ref,\n",
        "                \"prediction\": pred\n",
        "            })\n",
        "\n",
        "    pretrained_results_df = pd.DataFrame(records)\n",
        "    return pretrained_metric.compute(), predictions, pretrained_results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qprtX_4Gw6WY"
      },
      "source": [
        "Evaluate Execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bhBuDMyBl_Jo"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    f\"### Instruction:\\n{row['instruction']}\\n\\n### Response:\"\n",
        "    for _, row in test_sample.iterrows()\n",
        "]\n",
        "references = list(test_sample[\"output\"])\n",
        "\n",
        "baseline_scores, baseline_outputs, pretrained_results_df = evaluate_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts,\n",
        "    references\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nVhboxT12Q0R"
      },
      "outputs": [],
      "source": [
        "print(\"ROUGE Evaluation:\")\n",
        "for metric, score in baseline_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MuXlYUlx6mKf"
      },
      "outputs": [],
      "source": [
        "pretrained_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ87XpbLdBoH"
      },
      "source": [
        "### Step 4: Fine-tune the model on the chosen dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40wKV3P9OSM"
      },
      "source": [
        "Memory Cleanup and Re-initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V4CsEgvzSABt"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByjLVXqO9UES"
      },
      "source": [
        "Re-loads the tokenizer and re-downloads the dataset and selects the first 1,000 samples from the training set for fine-tuning.\n",
        "\n",
        "(Colab Notebook is restarted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "zQ_ESXW_IZ7k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "data = load_dataset(\"gbharti/finance-alpaca\")\n",
        "train_sample = data[\"train\"].select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZ3FrN79oCx"
      },
      "source": [
        "Reconfigure Quantization for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kFH_HvXgIpAR"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pAUOFcaNJTW3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d_XAgKxO7gVj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3bZFrRVsl7ip"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c2efv9B-LhX"
      },
      "source": [
        "Formats each training example into a structured prompt-response pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kof25lsQ62nr"
      },
      "outputs": [],
      "source": [
        "def build_prompt_completion(example):\n",
        "    if example[\"input\"]:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"completion\": example[\"output\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d-oDsr9SpDWS"
      },
      "outputs": [],
      "source": [
        "train_sample = train_sample.map(build_prompt_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3N0MEmZ-PyA"
      },
      "source": [
        "epares the dataset for training by tokenizing each example's prompt which includes both instruction and expected response.\n",
        "\n",
        "Truncates sequences longer than 512 tokens and pads shorter ones to that length for consistent batch shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0bDnKx976030"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(example):\n",
        "    tokenized = tokenizer(\n",
        "        example[\"prompt\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EzLnsfcQL7Tm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "output_directory = os.path.join(\".\", \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK2S0DV8-3CK"
      },
      "source": [
        "Defines the key hyperparameters and behaviors for fine-tuning the model using Hugging Face's Trainer or SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D1GpLUXcX-Ks"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_lab_outputs\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zCKkUtT_Cib"
      },
      "source": [
        "Creates a data collator that dynamically batches tokenized examples for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uOusVdWJ69oz"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-trORLTL_IBq"
      },
      "source": [
        "SFTTrainer is a wrapper around Hugging Face’s Trainer that supports parameter-efficient fine-tuning (PEFT) via LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zac5vtE37Cnm"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    peft_config=lora_config,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "8xFzjir47JQ9"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nB16MSRaO19K"
      },
      "outputs": [],
      "source": [
        "# Define output directory for saving\n",
        "output_directory = \"./peft_outputs\"\n",
        "peft_model_path = os.path.join(output_directory, \"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9vcp51o_PKM"
      },
      "source": [
        "### Step 5: Evaluate the fine-tuned model’s performance using the same metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dTNhbzA_g9zF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M4009CIoxp_M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0TUZISe_wvSS"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MvTZNS9b3CDK"
      },
      "outputs": [],
      "source": [
        "peft_model_path = \"./peft_outputs/lora_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eaWbr294wv_H"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yx18WClgmz-"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # valid for GPU\n",
        "    bnb_4bit_quant_type=\"nf4\",             # use 'nf4' for GPU compatibility\n",
        "    llm_int8_enable_fp32_cpu_offload=False # must be False when not offloading\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ovZ_hwePOlwu"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model + tokenizer from saved adapter\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_path,\n",
        "    device_map=\"auto\",\n",
        "    is_trainable=False,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rw3NLckvQSSx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test_sample = load_dataset(\"gbharti/finance-alpaca\", split=\"train[1000:1300]\")\n",
        "\n",
        "prompts = [\n",
        "    f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nResponse:\"\n",
        "    for ex in test_sample\n",
        "]\n",
        "references = [ex[\"output\"] for ex in test_sample]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dGNfwK4nvwcA"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "metric = load(\"rouge\")  # or use rouge_scorer if preferred\n",
        "results = []\n",
        "all_outputs = []\n",
        "\n",
        "batch_size = 4\n",
        "loaded_model.eval()\n",
        "\n",
        "for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "    batch_prompts = prompts[i:i+batch_size]\n",
        "    batch_refs = references[i:i+batch_size]\n",
        "\n",
        "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(loaded_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=128,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    all_outputs.extend(decoded_outputs)\n",
        "\n",
        "    for prompt_text, pred, ref in zip(batch_prompts, decoded_outputs, batch_refs):\n",
        "        metric.add(prediction=pred, reference=ref)\n",
        "        results.append({\n",
        "            \"prompt\": prompt_text,\n",
        "            \"reference\": ref,\n",
        "            \"prediction\": pred\n",
        "        })\n",
        "\n",
        "# Create final DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "rouge_scores = metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "V2uLjOPCrANo"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v8wV_ln1wPWY"
      },
      "outputs": [],
      "source": [
        "rouge_scores"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
